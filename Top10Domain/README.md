## 1. Äá»c dá»¯ liá»‡u tá»« PageViewApp 
- Äá»c dá»¯ liá»‡u Parquet tá»« thÆ° má»¥c `hdfs://adt-platform-dev-106-254:8120/data/Parquet/PageViewApp/` theo tá»«ng ngÃ y (dá»±a trÃªn tÃªn thÆ° má»¥c kiá»ƒu `YYYY_MM_DD`).
- TÃ­nh sá»‘ láº§n xuáº¥t hiá»‡n (`count`) cá»§a má»—i `appId` trong má»—i ngÃ y.
- Cá»™ng dá»“n káº¿t quáº£ qua cÃ¡c ngÃ y Ä‘á»ƒ táº¡o tá»•ng há»£p cuá»‘i cÃ¹ng.
- Hiá»ƒn thá»‹ káº¿t quáº£ tá»«ng ngÃ y vÃ  tá»•ng há»£p cuá»‘i cÃ¹ng theo thá»© tá»± giáº£m dáº§n cá»§a `count`.

## 2. Cáº¥u trÃºc Code
### 2.1. Khá»Ÿi táº¡o Spark Session
```scala
val spark = SparkSession.builder().getOrCreate()
import spark.implicits._
```
- Táº¡o hoáº·c láº¥y phiÃªn báº£n Spark Session hiá»‡n cÃ³ Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u.
- Import cÃ¡c hÃ m implict Ä‘á»ƒ há»— trá»£ thao tÃ¡c DataFrame.

### 2.2. Thiáº¿t láº­p Ä‘Æ°á»ng dáº«n vÃ  FileSystem
```scala
val baseDir = "hdfs://adt-platform-dev-106-254:8120/data/Parquet/PageViewApp/"
val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
```
- Äá»‹nh nghÄ©a Ä‘Æ°á»ng dáº«n cÆ¡ sá»Ÿ (`baseDir`) Ä‘áº¿n thÆ° má»¥c HDFS chá»©a dá»¯ liá»‡u Parquet.
- Láº¥y Ä‘á»‘i tÆ°á»£ng `FileSystem` Ä‘á»ƒ liá»‡t kÃª cÃ¡c file vÃ  thÆ° má»¥c.

### 2.3. Láº¥y danh sÃ¡ch thÆ° má»¥c ngÃ y
```scala
val dayDirs = fs.listStatus(new Path(baseDir))
  .map(_.getPath)
  .filter(p => p.getName.matches("\\d{4}_\\d{2}_\\d{2}"))
  .sortBy(_.getName)
```
- Láº¥y danh sÃ¡ch cÃ¡c thÆ° má»¥c con trong `baseDir`.
- Lá»c cÃ¡c thÆ° má»¥c cÃ³ tÃªn khá»›p vá»›i Ä‘á»‹nh dáº¡ng ngÃ y (`YYYY_MM_DD`).
- Sáº¯p xáº¿p theo thá»© tá»± tÄƒng dáº§n cá»§a tÃªn ngÃ y.

### 2.4. Xá»­ lÃ½ tá»«ng ngÃ y
```scala
var runningTotalDF: DataFrame = spark.emptyDataFrame

for (dayPath <- dayDirs) {
  val day = dayPath.getName
  println(s"\nğŸŸ¢ Äang xá»­ lÃ½ ngÃ y: $day")

  val parquetFiles = fs.listStatus(dayPath)
    .map(_.getPath.toString)
    .filter(_.endsWith(".parquet"))

  if (parquetFiles.isEmpty) {
    println(s"âš ï¸ KhÃ´ng cÃ³ file parquet trong $day")
  } else {
    val batchSize = 1
    val fileGroups = parquetFiles.grouped(batchSize).toList

    var dayCounts = scala.collection.mutable.Map[String, Long]()

    for ((group, idx) <- fileGroups.zipWithIndex) {
      println(s"   ğŸ“¦ NhÃ³m ${idx + 1}/${fileGroups.size}")
      val df = spark.read.parquet(group: _*)
        .select("appId")
        .groupBy("appId")
        .agg(count("*").as("count"))
        .collect()

      df.foreach { row =>
        val appId = row.getString(0)
        val count = row.getLong(1)
        dayCounts(appId) = dayCounts.getOrElse(appId, 0L) + count
      }
    }

    val reducedDayDF = dayCounts.toSeq.toDF("appId", "count")

    println(s"ğŸ“Š Káº¿t quáº£ cho ngÃ y $day:")
    reducedDayDF.orderBy(desc("count")).show(truncate = false)

    if (runningTotalDF.isEmpty) {
      runningTotalDF = reducedDayDF
    } else {
      val runningMap = runningTotalDF.collect().map(r => r.getString(0) -> r.getLong(1)).toMap
      val newDayMap = reducedDayDF.collect().map(r => r.getString(0) -> r.getLong(1)).toMap

      val merged = (runningMap.keySet ++ newDayMap.keySet).map { appId =>
        appId -> (runningMap.getOrElse(appId, 0L) + newDayMap.getOrElse(appId, 0L))
      }.toSeq

      runningTotalDF = merged.toDF("appId", "count")
    }
  }
}
```
- **VÃ²ng láº·p qua cÃ¡c ngÃ y**: Duyá»‡t qua tá»«ng thÆ° má»¥c ngÃ y.
- **Láº¥y file Parquet**: Liá»‡t kÃª cÃ¡c file `.parquet` trong thÆ° má»¥c ngÃ y.
- **Xá»­ lÃ½ theo batch**: Chia file thÃ nh cÃ¡c nhÃ³m (batch size = 1), xá»­ lÃ½ tá»«ng nhÃ³m Ä‘á»ƒ trÃ¡nh táº£i toÃ n bá»™ dá»¯ liá»‡u cÃ¹ng lÃºc.
- **TÃ­nh count**: Äá»c file, chá»n cá»™t `appId`, nhÃ³m vÃ  tÃ­nh tá»•ng (`count`), lÆ°u vÃ o `dayCounts` (Map mutable).
- **Táº¡o DataFrame ngÃ y**: Chuyá»ƒn `dayCounts` thÃ nh DataFrame (`reducedDayDF`) vÃ  hiá»ƒn thá»‹.
- **Cá»™ng dá»“n**: Náº¿u `runningTotalDF` rá»—ng, gÃ¡n `reducedDayDF`; náº¿u khÃ´ng, há»£p nháº¥t báº±ng cÃ¡ch cá»™ng cÃ¡c giÃ¡ trá»‹ `count` cá»§a `appId` giá»‘ng nhau.

### 2.5. Hiá»ƒn thá»‹ káº¿t quáº£ cuá»‘i cÃ¹ng
```scala
if (!runningTotalDF.isEmpty) {
  println("\nğŸ Káº¿t quáº£ tá»•ng há»£p toÃ n bá»™:")
  runningTotalDF.orderBy(desc("count")).show(100, truncate = false)
} else {
  println("ğŸš« KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½.")
}
```
- Hiá»ƒn thá»‹ DataFrame tá»•ng há»£p (`runningTotalDF`) theo thá»© tá»± giáº£m dáº§n cá»§a `count`, tá»‘i Ä‘a 100 dÃ²ng.
- Náº¿u khÃ´ng cÃ³ dá»¯ liá»‡u, in thÃ´ng bÃ¡o.

## 1. Äá»c dá»¯ liá»‡u tá»« PageViewMobile 
- Äá»c dá»¯ liá»‡u Parquet tá»« thÆ° má»¥c `hdfs://adt-platform-dev-106-254:8120/data/Parquet/PageViewMobile/` theo tá»«ng ngÃ y (dá»±a trÃªn tÃªn thÆ° má»¥c kiá»ƒu `YYYY_MM_DD`).
- TÃ­nh sá»‘ láº§n xuáº¥t hiá»‡n (`count`) cá»§a má»—i `domain` trong má»—i ngÃ y.
- Cá»™ng dá»“n káº¿t quáº£ qua cÃ¡c ngÃ y Ä‘á»ƒ táº¡o tá»•ng há»£p cuá»‘i cÃ¹ng.
- Hiá»ƒn thá»‹ káº¿t quáº£ tá»«ng ngÃ y vÃ  tá»•ng há»£p cuá»‘i cÃ¹ng theo thá»© tá»± giáº£m dáº§n cá»§a `count`.

- Thá»±c hiá»‡n tÆ°Æ¡ng tá»± Ä‘á»c App chá»‰ cáº§n thay Ä‘Æ°á»ng dáº«n thÆ° má»¥c

## 1. Äá»c dá»¯ liá»‡u tá»« PageViewV1 (PC)
- Äá»c dá»¯ liá»‡u Parquet tá»« thÆ° má»¥c `hdfs://adt-platform-dev-106-254:8120/data/Parquet/PageViewV1/` theo tá»«ng ngÃ y (dá»±a trÃªn tÃªn thÆ° má»¥c kiá»ƒu `YYYY_MM_DD`).
- TÃ­nh sá»‘ láº§n xuáº¥t hiá»‡n (`count`) cá»§a má»—i `domain` trong má»—i ngÃ y.
- Cá»™ng dá»“n káº¿t quáº£ qua cÃ¡c ngÃ y Ä‘á»ƒ táº¡o tá»•ng há»£p cuá»‘i cÃ¹ng.
- Hiá»ƒn thá»‹ káº¿t quáº£ tá»«ng ngÃ y vÃ  tá»•ng há»£p cuá»‘i cÃ¹ng theo thá»© tá»± giáº£m dáº§n cá»§a `count`.

- Thá»±c hiá»‡n tÆ°Æ¡gn tá»± Ä‘á»c App vÃ  Mobile chá»‰ cáº§n thay Ä‘Æ°á»ng dáº«n thÆ° má»¥c 
