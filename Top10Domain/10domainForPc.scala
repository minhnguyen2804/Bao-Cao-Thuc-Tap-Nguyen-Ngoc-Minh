{
import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.hadoop.fs.{FileSystem, Path}

val spark = SparkSession.builder().getOrCreate()
import spark.implicits._

val baseDir = "hdfs://adt-platform-dev-106-254:8120/data/Parquet/PageViewV1/"
val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)

// Láº¥y danh sÃ¡ch thÆ° má»¥c ngÃ y
val dayDirs = fs.listStatus(new Path(baseDir))
  .map(_.getPath)
  .filter(p => p.getName.matches("\\d{4}_\\d{2}_\\d{2}"))
  .sortBy(_.getName)

var runningTotalDF: DataFrame = spark.emptyDataFrame

for (dayPath <- dayDirs) {
  val day = dayPath.getName
  println(s"\nğŸŸ¢ Äang xá»­ lÃ½ ngÃ y: $day")

  val parquetFiles = fs.listStatus(dayPath)
    .map(_.getPath.toString)
    .filter(_.endsWith(".parquet"))

  if (parquetFiles.isEmpty) {
    println(s"âš ï¸ KhÃ´ng cÃ³ file parquet trong $day")
  } else {
    val batchSize = 10
    val fileGroups = parquetFiles.grouped(batchSize).toList

    var dayCounts = scala.collection.mutable.Map[String, Long]()

    for ((group, idx) <- fileGroups.zipWithIndex) {
      println(s"   ğŸ“¦ NhÃ³m ${idx + 1}/${fileGroups.size}")
      val df = spark.read.parquet(group: _*)
        .select("domain")
        .groupBy("domain")
        .agg(count("*").as("count"))
        .collect()

      df.foreach { row =>
        val domain = row.getString(0)
        val count = row.getLong(1)
        dayCounts(domain) = dayCounts.getOrElse(domain, 0L) + count
      }
    }

    // Táº¡o DataFrame cho ngÃ y Ä‘Ã³ tá»« Map
    val reducedDayDF = dayCounts.toSeq.toDF("domain", "count")

    println(s"ğŸ“Š Káº¿t quáº£ cho ngÃ y $day:")
    reducedDayDF.orderBy(desc("count")).show(truncate = false)

    // Cá»™ng dá»“n vá»›i tá»•ng
    if (runningTotalDF.isEmpty) {
      runningTotalDF = reducedDayDF
    } else {
      // convert vá» Map Ä‘á»ƒ cá»™ng dá»… hÆ¡n
      val runningMap = runningTotalDF.collect().map(r => r.getString(0) -> r.getLong(1)).toMap
      val newDayMap = reducedDayDF.collect().map(r => r.getString(0) -> r.getLong(1)).toMap

      val merged = (runningMap.keySet ++ newDayMap.keySet).map { domain =>
        domain -> (runningMap.getOrElse(domain, 0L) + newDayMap.getOrElse(domain, 0L))
      }.toSeq

      runningTotalDF = merged.toDF("domain", "count")
    }
  }
}

// Káº¿t quáº£ cuá»‘i cÃ¹ng
if (!runningTotalDF.isEmpty) {
  println("\nğŸ Káº¿t quáº£ tá»•ng há»£p toÃ n bá»™:")
  runningTotalDF.orderBy(desc("count")).show(10, truncate = false)
} else {
  println("ğŸš« KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½.")
}
}

